PyTorch - Deep Learning Framework

BytePair Encoding

(Core mechanics of the LLM [the car engine])
4 Attention Mechanisms:
1. Simplified self-attention (technique to introduce the broader idea)
2. Self-attention (wit trainable weights, forms the basis of the mechanism used in LLMs)
3. Causal attention (type of self-attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, 
ensuring temporal order during the text generation)
4. Multi-head attention (extension of self-attention and causal attention that enables the model to simultaneously attend to information
 from different representation subspaces)


Attention weights and context vectors... Chpt 2...


Masking, drop out mask
Note, drop out mask is not used by modern LLMs anymore.


(The Architecture is the rest of the car)

introduce non-linearity into the model, so you should pair  a linear layer with a non-linear activation function. Thus the network can learn more complex patterns.

Use Layer Nomralization, where it is independent of the sample size.

non activation functions, there are many that you can choose from...


Back propagation

Transformer blocks

Weight sharing... Some LLMs do it (LAMA 3.2), and some dont (LAMA 3 and LAMA 3.1)
If you share the weight, there are fewer parameters in the model because these are really large matrices 

output of the last linear layer is called logits.


GPT2-small (the 124M configuration we already implemented):
    - "emb_dim" = 768
    - "n_layers" = 12
    - "n_heads" = 12
    - GPT2-medium:

GPT2-medium:
    - "emb_dim" = 1024
    - "n_layers" = 24
    - "n_heads" = 16

GPT2-large:
    - "emb_dim" = 1280
    - "n_layers" = 36
    - "n_heads" = 20

GPT2-XL:
    - "emb_dim" = 1600
    - "n_layers" = 48
    - "n_heads" = 25




cross-entropy loss in deep learning (cross-entropy = negative average log probability)
In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value;
in our case, instead of maximizing -10.7722 so that it approaches 0,
in deep learning, we would minimize 10.7722 so that it approaches 0



perplexity: measured as the exponential of the loss (training loss/validation loss)-> essentially tells us how unsure the model is for the number of words.


in LLM training, one epoch is one complete pass over a training set.
Iterate over batches in each training epoch (the # of batches is determined by the training set size divided by the size of each batch).

For each iteration (usual steps used for training deep neural networks in PyTorch):
- Reset loss gradients from previous batch iteration
- calculate loss on current batch
- backward pass to calculate loss gradients
- update model weights using loss gradients
- print training and validation set losses


normally people train models for 1 or 1.5 epochs max (since its 15 trillion tokens, thousands of GPUs, lots of $$$, and months of time)
in this case, the verdict short story is a small text file so to get better results, the more epochs, the better results.


top-k is the an integer value that grabs the top K (n) highest logits. (e.g. k = 3, focus on the 3 most likely tokens or logits.) 